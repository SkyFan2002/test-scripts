use std::{fmt::Display, fs::File};

use arrow::array::{ArrayRef, RecordBatch};
use bytes::Bytes;
use parquet::{
    arrow::{arrow_reader::ParquetRecordBatchReaderBuilder, arrow_to_parquet_schema, ArrowWriter},
    basic::{Compression, Encoding, ZstdLevel},
    file::properties::{EnabledStatistics, WriterProperties},
    schema::types::SchemaDescriptor,
};
use serde::{Deserialize, Serialize};
pub const NUM_ROWS_PER_BLOCK: usize = 1000 * 1000;
/// should be set to the directory containing tpch tables, generated by https://github.com/wubx/databend-workshop/blob/main/databend_tpch/gen_data.py
const TPCH_DIR: &str = "";
/// should be set to the directory to store the result csv files
const RESULT_DIR: &str = "";

#[derive(Serialize, Deserialize, Debug)]
pub struct BenchColumnChunkResult {
    compress_time_ns: u128,
    compressed_bytes: i64,
    decompress_time_ns: u128,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct ReadableResult {
    codec: String,
    compress_time_ms: f64,
    compressed_kb: f64,
    decompress_time_ms: f64,
}

impl std::ops::Add for BenchColumnChunkResult {
    type Output = Self;
    fn add(self, other: Self) -> Self {
        BenchColumnChunkResult {
            compress_time_ns: self.compress_time_ns + other.compress_time_ns,
            compressed_bytes: self.compressed_bytes + other.compressed_bytes,
            decompress_time_ns: self.decompress_time_ns + other.decompress_time_ns,
        }
    }
}

impl std::iter::Sum for BenchColumnChunkResult {
    fn sum<I: Iterator<Item = Self>>(iter: I) -> Self {
        iter.fold(
            BenchColumnChunkResult {
                compress_time_ns: 0,
                compressed_bytes: 0,
                decompress_time_ns: 0,
            },
            |a, b| a + b,
        )
    }
}

#[derive(Clone, Copy)]
pub struct BenchColumnChunkSettings {
    pub encoding: Encoding,
    pub compression: Compression,
    pub dictionary_enabled: bool,
}

impl Display for BenchColumnChunkSettings {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let encoding = if self.dictionary_enabled {
            "dict"
        } else {
            match self.encoding {
                Encoding::PLAIN => "plain",
                Encoding::DELTA_LENGTH_BYTE_ARRAY => "delta length",
                Encoding::DELTA_BYTE_ARRAY => "delta",
                _ => unimplemented!(),
            }
        };
        match self.compression {
            Compression::UNCOMPRESSED => write!(f, "{}", encoding),
            Compression::ZSTD(_) => write!(f, "{} + zstd", encoding),
            _ => unimplemented!(),
        }
    }
}

/// Bench a single column chunk
pub fn bench_column_chunk(
    array: ArrayRef,
    column_bench_settings: BenchColumnChunkSettings,
) -> BenchColumnChunkResult {
    let BenchColumnChunkSettings {
        encoding,
        compression,
        dictionary_enabled,
    } = column_bench_settings;
    // Write data to parquet
    let settings = WriterProperties::builder()
        .set_compression(compression)
        .set_encoding(encoding)
        .set_dictionary_enabled(dictionary_enabled)
        .set_statistics_enabled(EnabledStatistics::None)
        .set_bloom_filter_enabled(false)
        .build();
    let batch = RecordBatch::try_from_iter(vec![("array", array.clone())]).unwrap();
    let mut buffer = vec![];
    let mut writer = ArrowWriter::try_new(&mut buffer, batch.schema(), Some(settings)).unwrap();
    let start = quanta::Instant::now();
    writer.write(&batch).unwrap();
    let file_meta = writer.close().unwrap();
    let compress_time = start.elapsed();
    let compressed_size = file_meta.row_groups[0].columns[0]
        .meta_data
        .as_ref()
        .unwrap()
        .total_compressed_size;
    // Read data from parquet
    let buffer = Bytes::from(buffer);
    let reader_builder = ParquetRecordBatchReaderBuilder::try_new(buffer)
        .unwrap()
        .with_batch_size(usize::MAX);
    let mut reader = reader_builder.build().unwrap();
    let start = quanta::Instant::now();
    let read_result = reader.next().unwrap().unwrap();
    assert_eq!(read_result.num_rows(), array.len());
    let decompress_time = start.elapsed();
    let compress_time_ns = compress_time.as_nanos();
    let decompress_time_ns = decompress_time.as_nanos();
    BenchColumnChunkResult {
        compress_time_ns,
        compressed_bytes: compressed_size,
        decompress_time_ns,
    }
}

/// a source directory is a tpch table, which contains multiple parquet files
pub fn bench_tpch_column(
    source_dir: &str,
    index: usize,
    settings: BenchColumnChunkSettings,
) -> BenchColumnChunkResult {
    let mut results = vec![];
    let paths = std::fs::read_dir(source_dir).unwrap();
    for path in paths {
        let path = path.unwrap().path();
        let file = File::open(&path).unwrap();
        let builder = ParquetRecordBatchReaderBuilder::try_new(file)
            .unwrap()
            .with_batch_size(NUM_ROWS_PER_BLOCK);
        let reader = builder.build().unwrap();
        for batch in reader.into_iter() {
            let batch = batch.unwrap();
            let column = batch.column(index);
            let result = bench_column_chunk(column.clone(), settings);
            results.push(result);
        }
    }
    results.into_iter().sum()
}

pub fn bench_tpch_column_ten_times(
    source_dir: &str,
    index: usize,
    settings: BenchColumnChunkSettings,
) -> (f64, f64, i64) {
    let mut results = vec![];
    for _ in 0..10 {
        let result = bench_tpch_column(source_dir, index, settings);
        results.push(result);
    }
    let result: BenchColumnChunkResult = results.into_iter().sum();
    let compress_time_ns = result.compress_time_ns as f64 / 10.0;
    let decompress_time_ns = result.decompress_time_ns as f64 / 10.0;
    let compressed_bytes = result.compressed_bytes / 10;
    (compress_time_ns, decompress_time_ns, compressed_bytes)
}

pub fn get_byte_array_index(source_dir: &str) -> (Vec<usize>, SchemaDescriptor) {
    let paths = std::fs::read_dir(source_dir).unwrap();
    let path = paths.into_iter().next().unwrap().unwrap().path();
    let file = File::open(path).unwrap();
    let builder = ParquetRecordBatchReaderBuilder::try_new(file).unwrap();
    let schema = builder.schema();
    let parquet_schema = arrow_to_parquet_schema(schema).unwrap();
    let mut byte_array_index = vec![];
    for (i, col) in parquet_schema.columns().iter().enumerate() {
        if matches!(
            col.physical_type(),
            parquet::basic::Type::BYTE_ARRAY | parquet::basic::Type::FIXED_LEN_BYTE_ARRAY
        ) {
            byte_array_index.push(i);
        }
    }
    (byte_array_index, parquet_schema)
}

pub fn bench_tpch_string() {
    let paths = std::fs::read_dir(TPCH_DIR).unwrap();
    let settings = bench_string_settings();
    for path in paths {
        let path = path.unwrap().path();
        let (indices, parquet_schema) = get_byte_array_index(path.to_str().unwrap());
        let path_name = path.file_name().unwrap().to_str().unwrap().to_string();
        let table_name = path_name.split('/').last().unwrap();
        for index in indices {
            let column_name = parquet_schema.columns()[index].name();
            let result_path = format!("{}/{}.{}.csv", RESULT_DIR, table_name, column_name);
            let mut csv_writer = csv::WriterBuilder::new()
                .has_headers(true)
                .from_path(result_path)
                .unwrap();
            let mut results = vec![];
            println!("Start bench table: {}, column: {}", table_name, column_name);
            for setting in &settings {
                let result = bench_tpch_column_ten_times(path.to_str().unwrap(), index, *setting);
                results.push(ReadableResult {
                    codec: setting.to_string(),
                    compress_time_ms: result.0 / 1_000_000.0,
                    compressed_kb: result.2 as f64 / 1024.0,
                    decompress_time_ms: result.1 / 1_000_000.0,
                });
            }
            for result in results {
                csv_writer.serialize(result).unwrap();
            }
        }
    }
}

fn bench_string_settings() -> Vec<BenchColumnChunkSettings> {
    let mut settings = vec![];
    settings.push(BenchColumnChunkSettings {
        encoding: Encoding::PLAIN,
        compression: Compression::UNCOMPRESSED,
        dictionary_enabled: false,
    });
    settings.push(BenchColumnChunkSettings {
        encoding: Encoding::PLAIN,
        compression: Compression::ZSTD(ZstdLevel::default()),
        dictionary_enabled: false,
    });
    settings.push(BenchColumnChunkSettings {
        encoding: Encoding::PLAIN,
        compression: Compression::UNCOMPRESSED,
        dictionary_enabled: true,
    });
    settings.push(BenchColumnChunkSettings {
        encoding: Encoding::PLAIN,
        compression: Compression::ZSTD(ZstdLevel::default()),
        dictionary_enabled: true,
    });
    settings.push(BenchColumnChunkSettings {
        encoding: Encoding::DELTA_BYTE_ARRAY,
        compression: Compression::UNCOMPRESSED,
        dictionary_enabled: false,
    });
    settings.push(BenchColumnChunkSettings {
        encoding: Encoding::DELTA_BYTE_ARRAY,
        compression: Compression::ZSTD(ZstdLevel::default()),
        dictionary_enabled: false,
    });
    settings.push(BenchColumnChunkSettings {
        encoding: Encoding::DELTA_LENGTH_BYTE_ARRAY,
        compression: Compression::UNCOMPRESSED,
        dictionary_enabled: false,
    });
    settings.push(BenchColumnChunkSettings {
        encoding: Encoding::DELTA_LENGTH_BYTE_ARRAY,
        compression: Compression::ZSTD(ZstdLevel::default()),
        dictionary_enabled: false,
    });
    settings
}
